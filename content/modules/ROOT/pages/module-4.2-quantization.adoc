:imagesdir: ../assets/images
= Model Quantization Pipeline

== Learning Objectives

By completing this exercise, you will:

* Understand how to automate model quantization using Kubeflow Pipelines
* Learn to create, compile, and deploy ML pipelines in OpenShift AI
* Gain experience with pipeline components and data flow management
* Learn pipeline parameter configuration and execution monitoring
* Compare automated pipeline versus manual quantization workflows

== Overview

This pipeline automates the model quantization process you completed manually in the previous lab. The pipeline handles model downloading from Hugging Face, quantization, S3 upload, and accuracy evaluation automatically through a series of connected components.

== Pipeline Overview

The pipeline consists of the following stages:

1. *Create PVC*: Creates a Persistent Volume Claim for storing model data
2. *Download Model*: Downloads the specified model from Hugging Face Hub
3. *Quantize Model*: Performs model quantization (supports int4 and int8 quantization)
4. *Upload Model*: Uploads the quantized model to a S3 (MinIO) storage location
5. *Evaluate Model*: Evaluates the quantized model's accuracy
6. *Delete PVC*: Cleans up by deleting the PVC after completion

== Prerequisites

Before starting this exercise, ensure you have:

* Completed the previous quantization lab (module-optimization-lab-1)
* OpenShift AI environment with configured pipeline server
* S3-compatible storage data connection set up in OpenShift AI
* Access to a workbench with GPU capabilities

== Storage Requirements

The pipeline creates a PVC with:

* Size: 30Gi
* Access Mode: ReadWriteMany
* Storage Class: standard

Make sure your cluster has the appropriate storage class available.

== Data Connection Setup

Before running the pipeline, configure the S3 storage connection:

1. Create a data connection in OpenShift AI pointing to your MinIO S3 storage. You can reuse the **minio-models** connection created in the previous lab.
2. The data connection requires these mandatory fields:
* **Connection name**: `minio-models` (hardcoded in the pipeline source file)
* **Access Key**: Your MinIO access key
* **Secret Key**: Your MinIO secret key  
* **Endpoint**: Your MinIO endpoint URL
* **Bucket**: Ensure the target bucket exists in MinIO before running the pipeline 

== Workbench Setup

You will reuse the workbench created in the previous model quantization lab for pipeline development.

NOTE: If you stopped the workbench after the previous lab, you need to **start** it again to modify and compile the pipeline.

[.bordershadow]
image::quantization-workbench-start.png[title="Start Workbench for Pipeline Development Environment", link=self, window=blank, width=100%]

* Open a terminal session in the workbench:
+
[.bordershadow]
image::quantization-create-terminal.png[title="Create Terminal Session in Jupyter Workbench", link=self, window=blank, width=100%]

* Install the required dependencies for creating the Kubeflow Pipeline YAML:
+
[source,bash]
----
pip install -U kfp==2.9.0 kfp-kubernetes==1.3.0
----
+
[.bordershadow]
image::quantization-install-kfp.png[title="Install Kubeflow Pipeline SDK Dependencies", link=self, window=blank, width=100%]

==== Validation Step
Verify successful installation:

* No error messages during pip install
* Check versions: `pip list | grep kfp`
* Confirm both packages are installed: `kfp==2.9.0` and `kfp-kubernetes==1.3.0`

== Building the Pipeline

* In the Jupyter workbench, open the `quantization_pipeline.py` file from `workshop_code/quantization/llm_compressor` directory.
* Review the pipeline definition to understand its components and data flow

=== Pipeline Architecture Overview

Before diving into individual components, let's understand the overall pipeline structure and data flow: 

[source, python]
----
@dsl.pipeline(...)
def quantization_pipeline(model_id, output_path, quantization_type):
    pvc = CreatePVC(...)
    download = download_model(...)
    quantize = quantize_model(...)
    upload = upload_model(...)
    evaluate = evaluate_model(...)
    delete_pvc = DeletePVC(...)
    # series of mounts, tolerations, dependencies, cleanup
----

=== Pipeline Key Characteristics

**Data Flow Architecture:**
```
HuggingFace → Download → PVC → Quantize → PVC → Upload to S3
                                ↓
                            Evaluate ← PVC
```

**Resource Management:**

* **PersistentVolumeClaim**: Created dynamically to persist model files across pipeline steps
* **GPU Scheduling**: Tolerations (`nvidia.com/gpu`) enable scheduling on GPU-enabled nodes
* **Shared Storage**: PVC mounted across all tasks ensures consistent data access
* **Task Sequencing**: download → quantize → (upload & evaluate in parallel) → delete PVC
* **Secret Management**: S3 credentials injected securely via `use_secret_as_env()`
* **GPU Resources**: Allocated specifically with `set_accelerator_type/limit` for quantization tasks

=== Pipeline Components Deep Dive

Let's examine each component in detail: 

=== `download_model` Component

[source,python]
----
@dsl.component(...):
def download_model(model_id: str, output_path: str):
    from huggingface_hub import snapshot_download
    snapshot_download(repo_id=model_id, local_dir=output_path)
    print("Model downloaded successfully from HF.")
----

**Purpose**: Downloads the specified model from Hugging Face Hub to the shared PVC storage.

**Key Functions**:

* Uses `snapshot_download` to fetch the complete model repository
* Downloads model weights, tokenizer, and configuration files
* Stores all artifacts in the shared PVC for subsequent pipeline steps
* Provides the foundation for the quantization process

=== `quantize_model` Component

[source,python]
----
@dsl.component(...):
def quantize_model(model_path: str, output_path: str, quantization_type: str):
    # 1) load HF model/tokenizer  
    # 2) gather calibration data from a dataset  
    # 3) build SmoothQuant + GPTQ pipeline, depending on `quantization_type`  
    # 4) call `oneshot()`  
    # 5) save compressed model + tokenizer
----

**Purpose**: Performs the core quantization process on the downloaded model.

**Key Functions**:

* **Model Loading**: Loads model and tokenizer with automatic device mapping (`device_map="auto"`)
* **Calibration Data**: Gathers sample data from HuggingFace datasets for quantization statistics
* **Quantization Recipe**: Applies W4A16 quantization using SmoothQuant + GPTQ techniques
* **Processing**: Executes `oneshot()` method for calibration and model compression
* **Output**: Saves compressed model artifacts with `save_compressed=True`

**Key Details**:

* Supports both `int4` and `int8` quantization types
* Uses GPU acceleration for faster processing
* Maintains model quality through careful calibration

=== `upload_model` Component

[source, python]
----
@dsl.component(...):
def upload_model(model_path: str, s3_path: str):
    # Uses boto3 with env secrets for S3 endpoint  
    # Walk through model_path folder and upload each file  
----

**Purpose**: Uploads the quantized model artifacts to S3-compatible storage.

**Key Functions**:

* **S3 Configuration**: Uses boto3 with credentials from mounted Kubernetes secrets
* **File Processing**: Iterates through all model files in the specified directory
* **Batch Upload**: Transfers model weights, tokenizer, and configuration files
* **Storage Organization**: Maintains file structure and naming conventions in S3

**Security**:

* Accesses S3 credentials securely via environment variables (`s3_host`, `s3_access_key`)
* Uses the `minio-models` secret configured in your data connection

=== `evaluate_model` Component

[source,python]
----
@dsl.component(...):
def evaluate_model(model_path: str):
    # Constructs 'lm_eval' vLLM shell command  
    # Runs GSM8K few-shot evaluation  
    # Captures and prints output
----

**Purpose**: Evaluates the quantized model's performance using standardized benchmarks.

**Key Functions**:

* **Benchmark Testing**: Runs GSM8K few-shot evaluation to measure model quality
* **Command Construction**: Builds `lm_eval` commands with vLLM backend for efficient inference
* **Performance Metrics**: Captures accuracy and performance statistics
* **Results Reporting**: Prints evaluation outputs for analysis

=== Pipeline Compilation Process

[source,python]
----
compiler.Compiler().compile(
    quantization_pipeline, 
    package_path='quantization_pipeline.yaml'
)
----

**Purpose**: Generates a deployable YAML specification for Argo-based execution in the Kubeflow Pipelines backend.

== Compiling the Pipeline

Follow these steps to compile the pipeline into a YAML file for OpenShift AI:

IMPORTANT: Before compiling, verify your data connection name. If you haven't used `minio-models` as your data connection name, you must update the line `secret_name = "minio-models"` in the pipeline code to match your actual data connection name (lowercase, spaces removed).

* In the terminal of the Jupyter workbench, open the `quantization_pipeline.py` file in your workbench
* Execute the pipeline compilation:
+
[source,bash]
----
python quantization_pipeline.py
----
+
[.bordershadow]
image::quantization-compile-pipeline.png[title="Execute Pipeline Compilation in Terminal", link=self, window=blank, width=100%]

=== Validation Step
Verify successful compilation:

* `quantization_pipeline.yaml` file is created in the current directory
* No error messages appear in the terminal output
* Check file contents: `ls -la quantization_pipeline.yaml`

* Download the generated `quantization_pipeline.yaml` file to your local machine:
+
[.bordershadow]
image::quantization-download-pipeline.png[title="Download Pipeline YAML File from Workbench", link=self, window=blank, width=100%]

* Once you have the pipeline file, stop the workbench to free resources:
+
[.bordershadow]
image::quantization-notebook-workbench-done.png[title="Access Workbench Actions Menu", link=self, window=blank, width=100%]
[.bordershadow]
image::quantization-notebook-workbench-stop.png[title="Stop Workbench to Free GPU Resources", link=self, window=blank, width=100%]

Confirm successful download and cleanup:

* Pipeline YAML file is saved to your local machine
* File opens and shows valid YAML structure
* Workbench is stopped and no longer consuming resources

== Running Your Pipeline

Follow these steps to import and execute the pipeline in OpenShift AI:

=== Pipeline Import Process

* Log into your OpenShift AI dashboard 
* In the project `quantization`, navigate to **Data Science Pipelines** → **Pipelines**
* Click **Import Pipeline**:
+
[.bordershadow]
image::quantization-import-pipeline.png[title="Import Pipeline Button in OpenShift AI", link=self, window=blank, width=100%]

* Enter a descriptive **Pipeline name**, such as: `Model Quantization Pipeline`
* Choose **Upload** and select your generated `quantization_pipeline.yaml` file:
+
[.bordershadow]
image::quantization-import-pipeline-select.png[title="Upload Pipeline YAML File for Import", link=self, window=blank, width=100%]

* Click **Import pipeline** to complete the import process
* Review the pipeline graph to verify all components are connected correctly:
+
[.bordershadow]
image::quantization-import-pipeline-graph.png[title="Pipeline Graph Showing Connected Components", link=self, window=blank, width=100%]

==== Validation Step
Verify successful pipeline import:

* Pipeline appears in the pipelines list with correct name
* Pipeline graph displays all 6 components (CreatePVC, Download, Quantize, Upload, Evaluate, DeletePVC)
* All components are properly connected with dependency arrows
* No import error messages are displayed

=== Pipeline Execution

* To start a pipeline run, click the **Actions** button and select **Create run**:
+
[.bordershadow]
image::quantization-import-pipeline-create-run.png[title="Create New Pipeline Run from Actions Menu", link=self, window=blank, width=100%]

* Configure the pipeline parameters in the run creation form:
** **Name**: Provide a descriptive run name, e.g., `quantization-granite-3.3-2b-instruct`
** **model_id**: HuggingFace model identifier (default: `ibm-granite/granite-3.3-2b-instruct`)
** **output_path**: Directory name for the quantized model (default: `granite-int4-pipeline`)
** **quantization_type**: Quantization method to apply (options: `int4` or `int8`, default: `int4`)
+
[.bordershadow]
image::quantization-import-pipeline-create-run-params.png[title="Pipeline Run Parameters Configuration", link=self, window=blank, width=100%]

* Click **Create run** to start the pipeline execution
* Monitor the pipeline progress until completion:
+
[.bordershadow]
image::quantization-pipeline-run-success.png[title="Successful Pipeline Execution Status", link=self, window=blank, width=100%]

==== Validation Step
Verify successful pipeline execution:

* All pipeline components show green "Succeeded" status
* No failed or skipped components in the pipeline graph
* Pipeline execution time is reasonable (typically 15-30 minutes)
* Check the logs of each component for any warning messages

=== Verifying Results

* Check the model accuracy evaluation results by inspecting the pipeline logs
* Access the MinIO S3 dashboard and verify that the quantized model has been uploaded successfully:
+
[.bordershadow]
image::quantization-pipeline-run-minio.png[title="Quantized Model Files in MinIO S3 Bucket", link=self, window=blank, width=100%]

==== Final Validation Step
Confirm successful model quantization and upload:

* Quantized model directory appears in S3 bucket with the specified `output_path` name
* Model files include weights, tokenizer, and configuration files
* Model files size (`*.safetensors`) are significantly smaller than the original model (indicating successful quantization)
* Model can be accessed and downloaded from S3 storage

=== Bonus exercises
- Make the dataset parameters (such as dataset name, split, and number of calibration samples) configurable in the pipeline instead of hardcoding them.
- Add support for the `fp8` quantization type. For implementation details, refer to the link:https://docs.vllm.ai/projects/llm-compressor/en/latest/examples/quantization_w8a8_fp8/[LLM Compressor quantization guide]
- Try different quantization schemes and methods to see if you can further improve model accuracy.
- Integrate MLflow to track and compare the results of your quantization experiments, including accuracy metrics. 
** You may refer to the sample implementation provided at `llm_compressor\lab2-bonus-output`

== Resource Cleanup

After completing the exercise, clean up resources to avoid unnecessary costs:

=== Automatic Cleanup (by pipeline)
* **PVC deletion**: Handled automatically by the pipeline's DeletePVC component
* **Temporary files**: Removed during pipeline execution

=== Manual Cleanup
* **Pipeline runs**: Delete old pipeline runs from OpenShift AI interface
* **Workbench**: Ensure workbench is stopped (completed earlier)
* Check that no orphaned PVCs remain: Navigate to **Storage** → **PersistentVolumeClaims**

IMPORTANT: The quantized models in S3 storage are your valuable outputs from this exercise. Only delete them if you're certain they're no longer needed.

