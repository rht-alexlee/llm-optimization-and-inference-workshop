apiVersion: v1
kind: Pod
metadata:
  labels:
    app: vllm
  name: vllm
  annotations:
    io.podman.annotations.cdi.devices: "nvidia.com/gpu=0"
spec:
  containers:
  - name: vllm
    image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2
    # rhaiis image is missing the chat-templates
    # https://issues.redhat.com/browse/AIPCC-1523
    command:
      - python
      - '-m'
      - vllm.entrypoints.openai.api_server
    args:
      - --port=8000
      - --model=/mnt/models
      - '--served-model-name=granite-3.2-2b-instruct'
      - --max-model-len=8192
      - --tensor-parallel-size=1
    env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      - name: VLLM_LOGGING_LEVEL
        value: "DEBUG"
    resources:
      requests:
        nvidia.com/gpu=all: 1
    ports:
      - containerPort: 8000
        hostPort: 8000
    securityContext:
      seLinuxOptions:
        type: "spc_t"
      runAsNonRoot: true
    volumeMounts:
      - name: modelcar-model
        mountPath: /mnt/models
  - name: modelcar
    image: 'quay.io/redhat-ai-services/modelcar-catalog:granite-3.2-2b-instruct'
    args:
      - sh
      - '-c'
      - sleep infinity
    volumeMounts:
      - name: modelcar-model
        mountPath: /models
  volumes:
  - name: modelcar-model
    emptyDir: {}
